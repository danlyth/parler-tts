{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankit/venvs/parler/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import AutocastKwargs, InitProcessGroupKwargs, TorchDynamoPlugin, set_seed\n",
    "from accelerate.utils.memory import release_memory\n",
    "from multiprocess import set_start_method\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "from parler_tts import (\n",
    "    ParlerTTSConfig,\n",
    "    ParlerTTSForConditionalGeneration,\n",
    ")\n",
    "from training.arguments import DataTrainingArguments, ModelArguments, ParlerTTSTrainingArguments\n",
    "from training.data_local import DataCollator, DatasetLocal\n",
    "from training.eval import compute_metrics\n",
    "from training.utils import get_last_checkpoint, log_metric, log_pred, rotate_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def main(json_path, temperature_override):\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, ParlerTTSTrainingArguments))\n",
    "    # NOTE change here to pass json path directly rather than use sys.argv\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(json_path))\n",
    "\n",
    "    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "    send_example_telemetry(\"run_parler_tts\", model_args, data_args)\n",
    "\n",
    "    if training_args.dtype == \"float16\":\n",
    "        mixed_precision = \"fp16\"\n",
    "    elif training_args.dtype == \"bfloat16\":\n",
    "        mixed_precision = \"bf16\"\n",
    "    else:\n",
    "        mixed_precision = \"no\"\n",
    "\n",
    "    if data_args.pad_to_max_length and (\n",
    "        data_args.max_audio_token_length is None\n",
    "        or data_args.max_prompt_token_length is None\n",
    "        or data_args.max_description_token_length is None\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"`pad_to_max_length` is `True` but one of the following parameters has not been set: `max_audio_token_length`, `max_prompt_token_length`, `max_description_token_length`\"\n",
    "        )\n",
    "\n",
    "    padding = \"max_length\" if data_args.pad_to_max_length else \"longest\"\n",
    "\n",
    "    # Detecting last checkpoint and eventually continue from last checkpoint\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    # Accelerator preparation\n",
    "    kwargs_handlers = [InitProcessGroupKwargs(timeout=timedelta(minutes=60))]\n",
    "    if training_args.torch_compile:\n",
    "        # TODO(YL): add more compile modes?\n",
    "        kwargs_handlers.append(TorchDynamoPlugin(backend=\"inductor\", mode=\"default\"))  # reduce-overhead\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n",
    "        mixed_precision=mixed_precision,\n",
    "        log_with=training_args.report_to,\n",
    "        project_dir=training_args.output_dir,\n",
    "        # dispatch_batches=False,  # TODO (Dan) testing this as our batches are not all the same length # NOTE commenteing this out as a test\n",
    "        # split_batches=True,  # NOTE testing this\n",
    "        kwargs_handlers=kwargs_handlers,\n",
    "    )\n",
    "\n",
    "    accelerator.init_trackers(\n",
    "        project_name=data_args.wandb_project,\n",
    "        init_kwargs={\"wandb\": {\"save_code\": True}, \"name\": data_args.wandb_run_name},\n",
    "        config={**vars(training_args), **vars(data_args), **vars(model_args), \"name\": data_args.wandb_run_name},\n",
    "    )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    logger.setLevel(logging.INFO if accelerator.is_main_process else logging.WARN)\n",
    "\n",
    "    if not accelerator.is_main_process:\n",
    "        warnings.filterwarnings(\"ignore\")  # TODO, perhaps duplicating here\n",
    "\n",
    "    # Log a small summary on each proces\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n",
    "        f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "\n",
    "    # Set the verbosity to info of the Transformers logger (on main process only)\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed before initializing model.\n",
    "    # NOTE removed this\n",
    "    # set_seed(training_args.seed)\n",
    "\n",
    "    # 1. First, let's instantiate the tokenizers and model\n",
    "    # Note for distributed training, the .from_pretrained methods guarantee that only\n",
    "    # one local process can concurrently download model & vocab.\n",
    "\n",
    "    sample_rate = model_args.discrete_audio_feature_sample_rate\n",
    "\n",
    "    # load prompt tokenizer\n",
    "    prompt_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.prompt_tokenizer_name or model_args.description_tokenizer_name or model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        token=data_args.token,\n",
    "        trust_remote_code=data_args.trust_remote_code,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "        padding_side=\"left\",  # prompt has to be padded on the left bc it's preprend to codebooks hidden states\n",
    "    )\n",
    "\n",
    "    if model_args.use_fast_tokenizer:\n",
    "        logger.warning(\n",
    "            \"Disabling fast tokenizer warning: https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py#L3231-L3235\"\n",
    "        )\n",
    "        prompt_tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    # load audio reference encoder\n",
    "    audio_ref_encoder = AutoModel.from_pretrained(model_args.audio_ref_encoder_name, output_hidden_states=True)\n",
    "    audio_ref_encoder.to(training_args.device)\n",
    "    audio_ref_encoder.eval()\n",
    "    if model_args.audio_ref_encoder_hidden_layer is not None:\n",
    "        logger.info(f\"Using hidden layer {model_args.audio_ref_encoder_hidden_layer}\")\n",
    "\n",
    "    # 3. Next, let's load the config.\n",
    "    config = ParlerTTSConfig.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        token=data_args.token,\n",
    "        trust_remote_code=data_args.trust_remote_code,\n",
    "    )\n",
    "\n",
    "    # update pad token id and decoder_start_token_id\n",
    "    config.update(\n",
    "        {\n",
    "            \"pad_token_id\": model_args.pad_token_id if model_args.pad_token_id is not None else config.pad_token_id,\n",
    "            \"decoder_start_token_id\": model_args.decoder_start_token_id\n",
    "            if model_args.decoder_start_token_id is not None\n",
    "            else config.decoder_start_token_id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    model = ParlerTTSForConditionalGeneration.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        config=config,\n",
    "        token=data_args.token,\n",
    "        trust_remote_code=data_args.trust_remote_code,\n",
    "    )\n",
    "\n",
    "    # enable gradient checkpointing if necessary\n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Freeze Encoders\n",
    "    model.freeze_encoders(model_args.freeze_text_encoder)\n",
    "\n",
    "    audio_encoder_bos_token_id = model.generation_config.decoder_start_token_id\n",
    "    audio_encoder_eos_token_id = config.decoder.eos_token_id\n",
    "\n",
    "    logger.info(\"Loading datasets...\")\n",
    "    # Instantiate custom data collator\n",
    "    data_collator = DataCollator(\n",
    "        prompt_tokenizer=prompt_tokenizer,\n",
    "        pad_to_multiple_of=data_args.pad_to_multiple_of,\n",
    "        padding=padding,\n",
    "        prompt_max_length=data_args.max_prompt_token_length,\n",
    "        # description_max_length=data_args.max_description_token_length,\n",
    "        audio_max_length=data_args.max_audio_token_length,\n",
    "        audio_ref_max_length=data_args.max_audio_ref_length,\n",
    "    )\n",
    "\n",
    "    # NOTE change from here\n",
    "    generate_dataset_local = DatasetLocal(\n",
    "        root_audio_dir=data_args.finetune_audio_dir,\n",
    "        root_dac_dir=data_args.finetune_code_dir,\n",
    "        metadata_path=data_args.finetune_generate_metadata_path,\n",
    "        prompt_tokenizer=prompt_tokenizer,\n",
    "        audio_sr=model_args.audio_ref_encoder_sr,\n",
    "        audio_ref_len=model_args.audio_ref_len,\n",
    "        audio_ref_percentage=model_args.audio_ref_percentage,\n",
    "        num_codebooks=model_args.num_codebooks,\n",
    "        audio_encoder_bos_token_id=audio_encoder_bos_token_id,\n",
    "        audio_encoder_eos_token_id=audio_encoder_eos_token_id,\n",
    "        use_same_file_ref=data_args.finetune_use_same_file_ref,\n",
    "        use_precomputed_ref_embed=data_args.finetune_use_precomputed_ref_embed,\n",
    "    )\n",
    "\n",
    "    if data_args.max_generate_samples is not None:\n",
    "        indices = random.sample(range(len(generate_dataset_local)), data_args.max_generate_samples)\n",
    "        generate_dataset_local = Subset(generate_dataset_local, indices)\n",
    "\n",
    "    generate_dataloader = DataLoader(\n",
    "        generate_dataset_local,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=data_args.per_device_generate_batch_size,\n",
    "        drop_last=False,  # NOTE was True\n",
    "        num_workers=training_args.dataloader_num_workers,\n",
    "        pin_memory=training_args.dataloader_pin_memory,\n",
    "    )\n",
    "\n",
    "    # NOTE change from here\n",
    "\n",
    "    # T5 doesn't support fp16\n",
    "    autocast_kwargs = AutocastKwargs(enabled=(mixed_precision != \"fp16\"))\n",
    "\n",
    "    logger.info(\"Testing dataloaders\")\n",
    "    logger.info(f\"Number of generation samples: {len(generate_dataloader)}\")\n",
    "    for batch in generate_dataloader:\n",
    "        break\n",
    "    logger.info(\"Generation data example\")\n",
    "    for key, value in batch.items():\n",
    "        logger.info(f\"{key}: {value.shape}\")\n",
    "\n",
    "    # Prepare everything with accelerate\n",
    "    model = accelerator.prepare(model)\n",
    "    generate_dataloader = accelerator.prepare(generate_dataloader)\n",
    "\n",
    "    logger.info(\"AFTER preparing with accelerate\")\n",
    "    logger.info(\"Testing dataloaders\")\n",
    "    logger.info(f\"Number of generation samples: {len(generate_dataloader)}\")\n",
    "    for batch in generate_dataloader:\n",
    "        break\n",
    "    logger.info(\"Generation data example\")\n",
    "    for key, value in batch.items():\n",
    "        logger.info(f\"{key}: {value.shape}\")\n",
    "\n",
    "    # NOTE moved this up from further below\n",
    "\n",
    "    # NOTE added this override\n",
    "    if temperature_override is None:\n",
    "        temperature = (model_args.temperature,)\n",
    "    else:\n",
    "        temperature = (temperature_override,)\n",
    "    gen_kwargs = {\n",
    "        \"do_sample\": model_args.do_sample,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_length\": model_args.max_length,\n",
    "        # Because of the delayed pattern mask, generation might stop earlier because of unexpected behaviour\n",
    "        # on the first tokens of the codebooks that are delayed.\n",
    "        # This fix the issue.\n",
    "        \"min_new_tokens\": model_args.num_codebooks + 1,\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Updated gen_kwargs: {gen_kwargs}\")\n",
    "\n",
    "    def get_ref_embeddings(batch, accelerator):\n",
    "        with accelerator.autocast(autocast_handler=autocast_kwargs):\n",
    "            with torch.no_grad():\n",
    "                encoder_outputs = audio_ref_encoder(batch[\"audio_ref\"], batch[\"audio_ref_attention_mask\"])\n",
    "            if model_args.audio_ref_encoder_hidden_layer is not None:\n",
    "                hidden_layer = model_args.audio_ref_encoder_hidden_layer\n",
    "                encoder_outputs = encoder_outputs.hidden_states[hidden_layer]\n",
    "            else:\n",
    "                encoder_outputs = encoder_outputs.last_hidden_state\n",
    "            if model_args.audio_ref_encoder_mean_pooling:\n",
    "                encoder_outputs = torch.mean(encoder_outputs, dim=1)\n",
    "            encoder_outputs = BaseModelOutput(encoder_outputs)\n",
    "            # Size of encoder_outputs.last_hidden_state is (batch_size, audio_ref length / downsampling, hidden_size)\n",
    "            # Check that batch[\"attention_mask\"] is the size as encoder_outputs and crop/pad as necessary\n",
    "            if \"attention_mask\" in batch and not model_args.audio_ref_encoder_mean_pooling:\n",
    "                attention_mask = batch[\"attention_mask\"]\n",
    "                encoder_outputs_len = encoder_outputs.last_hidden_state.size(1)\n",
    "                attention_mask_len = attention_mask.size(1)\n",
    "                # attention_mask shape is (batch_size, 1, audio_ref length / downsampling)\n",
    "                # however, this mask isn't always exactly the same length as the encoder_outputs\n",
    "                if encoder_outputs_len < attention_mask_len:\n",
    "                    attention_mask = attention_mask[:, :encoder_outputs_len]\n",
    "                if encoder_outputs_len > attention_mask_len:\n",
    "                    pad_length = encoder_outputs_len - attention_mask_len\n",
    "                    pad = torch.zeros(attention_mask.size(0), pad_length, device=accelerator.device)\n",
    "                    attention_mask = torch.cat([attention_mask, pad], dim=-1)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "        return encoder_outputs, attention_mask\n",
    "\n",
    "    def generate_step(\n",
    "        model,\n",
    "        batch,\n",
    "        accelerator,\n",
    "        autocast_kwargs,\n",
    "    ):\n",
    "        if training_args.torch_compile:\n",
    "            model = model._orig_mod\n",
    "\n",
    "        model = accelerator.unwrap_model(model, keep_fp32_wrapper=mixed_precision != \"fp16\")\n",
    "        model.eval()\n",
    "        # TODO - move this \"to device\" eleswhere\n",
    "        for k, v in batch.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                batch[k] = v.to(accelerator.device)\n",
    "        if \"encoder_outputs\" not in batch:\n",
    "            encoder_outputs, attention_mask = get_ref_embeddings(batch, accelerator)\n",
    "            batch[\"encoder_outputs\"] = encoder_outputs\n",
    "            batch[\"attention_mask\"] = attention_mask\n",
    "        else:\n",
    "            batch[\"encoder_outputs\"] = BaseModelOutput(batch[\"encoder_outputs\"])\n",
    "\n",
    "        audio_refs = batch.pop(\"audio_ref\", None)\n",
    "        batch.pop(\"audio_ref_attention_mask\", None)\n",
    "        batch.pop(\"decoder_attention_mask\", None)\n",
    "\n",
    "        output_audios = model.generate(**batch, **gen_kwargs)\n",
    "        output_audios = accelerator.pad_across_processes(output_audios, dim=1, pad_index=0)\n",
    "        return output_audios, audio_refs\n",
    "\n",
    "    eval_metrics = []\n",
    "    eval_preds = []\n",
    "    eval_refs = []\n",
    "    eval_prompts = []\n",
    "    eval_start = time.time()\n",
    "\n",
    "    generation_count = 0\n",
    "    logger.info(\"***** Running generation *****\")\n",
    "    # only run on main process\n",
    "    # with accelerator.main_process_first():\n",
    "    for batch in tqdm(\n",
    "        generate_dataloader,\n",
    "        desc=\"Evaluating - Generation ...\",\n",
    "        position=2,\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    ):\n",
    "        generated_audios, audio_refs = generate_step(model, batch, accelerator, autocast_kwargs)\n",
    "        # Gather all predictions and targets\n",
    "        generated_audios, prompts = accelerator.pad_across_processes(\n",
    "            (generated_audios, batch[\"prompt_input_ids\"]), dim=1, pad_index=0\n",
    "        )\n",
    "        generated_audios, prompts = accelerator.gather_for_metrics((generated_audios, prompts))\n",
    "        eval_preds.extend(generated_audios)\n",
    "        eval_prompts.extend(prompts.to(\"cpu\"))\n",
    "        eval_refs.extend(audio_refs)\n",
    "        # generation_count += len(generated_audios)\n",
    "        # if generation_count >= 100:  # TODO remove hard-coded value (wandb can only do 100)\n",
    "        # break\n",
    "\n",
    "    eval_time = time.time() - eval_start\n",
    "\n",
    "    # compute metrics\n",
    "    if training_args.predict_with_generate:\n",
    "        # Just use the main process to compute the metrics\n",
    "        metric_values, pred_prompts, audios, transcriptions = compute_metrics(\n",
    "            eval_preds,\n",
    "            eval_refs,\n",
    "            eval_prompts,\n",
    "            model_args.asr_model_name_or_path,\n",
    "            data_args.per_device_generate_batch_size,\n",
    "            prompt_tokenizer,\n",
    "            sample_rate,\n",
    "            accelerator.device,\n",
    "        )\n",
    "\n",
    "        eval_metrics = metric_values\n",
    "        if \"wandb\" in training_args.report_to:\n",
    "            log_pred(\n",
    "                accelerator,\n",
    "                pred_prompts,\n",
    "                transcriptions,\n",
    "                audios,\n",
    "                sampling_rate=sample_rate,\n",
    "                step=1,\n",
    "                prefix=\"eval\",\n",
    "            )\n",
    "\n",
    "    log_metric(\n",
    "        accelerator,\n",
    "        metrics=eval_metrics,\n",
    "        train_time=eval_time,\n",
    "        step=1,\n",
    "        epoch=1,\n",
    "        prefix=\"eval\",\n",
    "    )\n",
    "\n",
    "    # release eval batch and relax metrics\n",
    "    eval_metrics = []\n",
    "    eval_preds = []\n",
    "    eval_prompts = []\n",
    "    batch = release_memory(batch)\n",
    "\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "ename": "CommError",
     "evalue": "Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m temperatures \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.6\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temperature \u001b[38;5;129;01min\u001b[39;00m temperatures:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 66\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(json_path, temperature_override)\u001b[0m\n\u001b[1;32m     54\u001b[0m     kwargs_handlers\u001b[38;5;241m.\u001b[39mappend(TorchDynamoPlugin(backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# reduce-overhead\u001b[39;00m\n\u001b[1;32m     56\u001b[0m accelerator \u001b[38;5;241m=\u001b[39m Accelerator(\n\u001b[1;32m     57\u001b[0m     gradient_accumulation_steps\u001b[38;5;241m=\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps,\n\u001b[1;32m     58\u001b[0m     mixed_precision\u001b[38;5;241m=\u001b[39mmixed_precision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     kwargs_handlers\u001b[38;5;241m=\u001b[39mkwargs_handlers,\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_trackers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwandb_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwandb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwandb_run_name\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwandb_run_name\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Setup logging\u001b[39;00m\n\u001b[1;32m     73\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m     datefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m     handlers\u001b[38;5;241m=\u001b[39m[logging\u001b[38;5;241m.\u001b[39mStreamHandler(sys\u001b[38;5;241m.\u001b[39mstdout)],\n\u001b[1;32m     77\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/parler/lib/python3.10/site-packages/accelerate/accelerator.py:687\u001b[0m, in \u001b[0;36mAccelerator.on_main_process.<locals>._inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPartialState\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_main_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/parler/lib/python3.10/site-packages/accelerate/accelerator.py:2574\u001b[0m, in \u001b[0;36mAccelerator.init_trackers\u001b[0;34m(self, project_name, config, init_kwargs)\u001b[0m\n\u001b[1;32m   2570\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   2571\u001b[0m                 tracker_init(project_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogging_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mstr\u001b[39m(tracker), {}))\n\u001b[1;32m   2572\u001b[0m             )\n\u001b[1;32m   2573\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2574\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtracker_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtracker\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2576\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tracker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers:\n",
      "File \u001b[0;32m~/venvs/parler/lib/python3.10/site-packages/accelerate/tracking.py:81\u001b[0m, in \u001b[0;36mon_main_process.<locals>.execute_on_main_process\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PartialState()\u001b[38;5;241m.\u001b[39mon_main_process(function)(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/parler/lib/python3.10/site-packages/accelerate/tracking.py:298\u001b[0m, in \u001b[0;36mWandBTracker.__init__\u001b[0;34m(self, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_name \u001b[38;5;241m=\u001b[39m run_name\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized WandB project \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    300\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure to log any initial configurations with `self.store_init_configuration` before training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/parler/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1178\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, settings)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[0;32m-> 1178\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "File \u001b[0;32m~/venvs/parler/lib/python3.10/site-packages/wandb/analytics/sentry.py:155\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/venvs/parler/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1164\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, settings)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[1;32m   1163\u001b[0m     wi\u001b[38;5;241m.\u001b[39msetup(kwargs)\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/parler/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:776\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m         backend\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[0;32m--> 776\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m run_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_result\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mCommError\u001b[0m: Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n"
     ]
    }
   ],
   "source": [
    "json_path = \"/home/ankit/code/parler-tts/helpers/training_configs/finetune_maya/maya_inference.json\"\n",
    "\n",
    "temperatures = [1.0, 1.0, 1.0, 0.9, 0.9, 0.9, 0.8, 0.8, 0.8, 0.7, 0.7, 0.7, 0.6, 0.6, 0.6]\n",
    "for temperature in temperatures:\n",
    "    main(json_path, temperature_override=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
